{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97816ed9",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Use of \"frequentist\"\n",
    "\n",
    "In this chapter, \"frequentist\" is often equated with hypothesis testing based on P-values.\n",
    "However, that's a bit misleading.\n",
    "Fundamentally, there are 2 branches statistics (frequentist and Bayesian), because we have two things we can condition on, the model or the data.\n",
    "For a nice explanation of this grounded in decision theory, check out the first 20 minutes of this lecture by Michael Jordan (no, not \"air\" Jordan, more like \"err\" Jordan):\n",
    "\n",
    "http://videolectures.net/mlss09uk_jordan_bfway/\n",
    "\n",
    "If your browser has trouble with this site, here's a YouTube version with no slides:\n",
    "\n",
    "https://www.youtube.com/watch?v=HUAE26lNDuE\n",
    "\n",
    "You can approach hypothesis testing from a frequentist or Bayesian approach.\n",
    "We tend to teach the frequentist approach (P-values and confidence intervals) to hypothesis testing, but they are not equivalent.\n",
    "\n",
    "### Comparing Bayes factors to AIC\n",
    "\n",
    "At the end of the third paragraph of Section 2.4c, when comparing model choice using AIC and Bayes factors, it reads \"If all parameters can be estimated with precision, results from both approaches should be similar.\"\n",
    "\n",
    "This is not always true. If the data are very informative, we can expect that our posterior probability densities of the parameters will be precise and robust to prior assumptions within each model. However, when comparing posterior probabilities across models, it is the average (marginal) likelihood of each model that updates prior probabilities. These average likelihoods are weighted by the prior. So, even if we have very informative data and precise parameter estimates within each model, the model posterior probabilities can still be very sensitive to the prior distributions on the parameters of each model.\n",
    "\n",
    "Long story short, if we put enough prior weight on unreasonable parameter values, we can \"sink\" the marginal likelihood of any model. This is responsible for what is known as [Lindley's Paradox](https://en.wikipedia.org/wiki/Lindley%27s_paradox).\n",
    "\n",
    "This doesn't mean that Bayesian model comparison is bad. It simply means that the priors we place on each parameter within each model serve as a \"penalty\" for that parameter, so we have to choose those priors carefully.\n",
    "\n",
    "\n",
    "## Errata\n",
    "\n",
    "### Equation 2.7\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Delta = -2 \\ln \\frac{\\textrm{ML}_1}{\\textrm{ML}_2} = -2 (\\ln \\textrm{ML}_1 - \\ln \\textrm{ML}_2)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Equation 2.21\n",
    "\n",
    "\"H\" should be \"D\" in the last equation. Also $p_h$ should be $p_H$:\n",
    "\n",
    "\\begin{equation}\n",
    "    Pr(H|D) = \\frac{P(D | p_H, N) f(p_H)}{\\int_{0}^{1} P(D | p_H, N) f(p_H) dp_H}\n",
    "\\end{equation}\n",
    "\n",
    "### MCMC 3b\n",
    "\n",
    "The proposal density is inverted. It should be:\n",
    "\n",
    "\\begin{equation}\n",
    "    R_{\\textrm{proposal}} = \\frac{Q(p | p^{\\prime})}{Q(p^{\\prime} | p)}\n",
    "\\end{equation}\n",
    "\n",
    "The way I remember this: If the proposing the new state is super likely, I need to penalize the acceptance ratio for it, so it needs to go in the denominator.\n",
    "\n",
    "### Last paragraph of Section 2.4c\n",
    "\n",
    "\"arrogance sampling\" should be \"importance sampling.\" Though, I must say, I am now partial to arrogance sampling.\n",
    "\n",
    "\n",
    "# Chapter 3\n",
    "\n",
    "## Notes\n",
    "\n",
    "Paragraph 2 of Section 3.3a reads (in reference to Brownian motion):\n",
    "\n",
    "\"This model assumes that the number of alleles is so large that there is effectively no chance of mutations happening to the same allele more than once...\"\n",
    "\n",
    "This is a bit confusing. At any given time, the model does not assume there must be a huge number of alleles. It only assumes that every mutation creates a new allele (it can't create an allele that has already been seen). So, as time approaches infinity, the total number of alleles observed (including the ones lost to drift, which is most of them) will approach infinity.\n",
    "\n",
    "## Errata\n",
    "\n",
    "### Equation 3.17\n",
    "This distribution should not be for the expectation of $\\bar{z}(t)$. It should be:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bar{z}(t) \\sim N(\\bar{z}(0), \\sigma_{B}^{2} t_1)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "# Chapter 4\n",
    "\n",
    "## Errata\n",
    "\n",
    "### In the last paragraph of Section 4.2\n",
    "\n",
    "$e^{2.68} = 2.3$ should be $e^{0.85} = 2.3$\n",
    "\n",
    "### Second sentence after Equation 4.9\n",
    "\n",
    "\"Equation 4.8\" should be \"Equation 4.9,\" so that it reads:\n",
    "\n",
    "\"Equation 4.9 is exactly identical to the estimated rate fo evolution calculated using the average squared independent contract...\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
